from models.tool_model import ToolInfoJson
import os
from services.db_service import db_service
from .StreamProcessor import StreamProcessor
from .agent_manager import AgentManager
import traceback
from utils.http_client import HttpClient, get_http_client
from langgraph_swarm import create_swarm  # type: ignore
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_google_genai import ChatGoogleGenerativeAI  # å¯¼å…¥Google Geminiä¸“ç”¨æ¨¡å‹
from services.websocket_service import send_to_websocket  # type: ignore
from services.config_service import config_service
from typing import Optional, List, Dict, Any, cast, Set, TypedDict
from models.config_model import ModelInfo
from utils.error_handler import clean_error_message


class ContextInfo(TypedDict):
    """Context information passed to tools"""
    canvas_id: str
    session_id: str
    model_info: Dict[str, List[ModelInfo]]


def _fix_chat_history(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ä¿®å¤èŠå¤©å†å²ä¸­ä¸å®Œæ•´çš„å·¥å…·è°ƒç”¨

    æ ¹æ®LangGraphæ–‡æ¡£å»ºè®®ï¼Œç§»é™¤æ²¡æœ‰å¯¹åº”ToolMessageçš„tool_calls
    å‚è€ƒ: https://langchain-ai.github.io/langgraph/troubleshooting/errors/INVALID_CHAT_HISTORY/
    """
    if not messages:
        return messages

    fixed_messages: List[Dict[str, Any]] = []
    tool_call_ids: Set[str] = set()

    # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰ToolMessageçš„tool_call_id
    for msg in messages:
        if msg.get('role') == 'tool' and msg.get('tool_call_id'):
            tool_call_id = msg.get('tool_call_id')
            if tool_call_id:
                tool_call_ids.add(tool_call_id)

    # ç¬¬äºŒéï¼šä¿®å¤AIMessageä¸­çš„tool_calls
    for msg in messages:
        if msg.get('role') == 'assistant' and msg.get('tool_calls'):
            # è¿‡æ»¤æ‰æ²¡æœ‰å¯¹åº”ToolMessageçš„tool_calls
            valid_tool_calls: List[Dict[str, Any]] = []
            removed_calls: List[str] = []

            for tool_call in msg.get('tool_calls', []):
                tool_call_id = tool_call.get('id')
                if tool_call_id in tool_call_ids:
                    valid_tool_calls.append(tool_call)
                elif tool_call_id:
                    removed_calls.append(tool_call_id)

            # è®°å½•ä¿®å¤ä¿¡æ¯
            if removed_calls:
                print(
                    f"ğŸ”§ ä¿®å¤æ¶ˆæ¯å†å²ï¼šç§»é™¤äº† {len(removed_calls)} ä¸ªä¸å®Œæ•´çš„å·¥å…·è°ƒç”¨: {removed_calls}")

            # æ›´æ–°æ¶ˆæ¯
            if valid_tool_calls:
                msg_copy = msg.copy()
                msg_copy['tool_calls'] = valid_tool_calls
                fixed_messages.append(msg_copy)
            elif msg.get('content'):  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„tool_callsä½†æœ‰contentï¼Œä¿ç•™æ¶ˆæ¯
                msg_copy = msg.copy()
                msg_copy.pop('tool_calls', None)  # ç§»é™¤ç©ºçš„tool_calls
                fixed_messages.append(msg_copy)
            # å¦‚æœæ—¢æ²¡æœ‰æœ‰æ•ˆtool_callsä¹Ÿæ²¡æœ‰contentï¼Œè·³è¿‡è¿™æ¡æ¶ˆæ¯
        else:
            # éassistantæ¶ˆæ¯æˆ–æ²¡æœ‰tool_callsçš„æ¶ˆæ¯ç›´æ¥ä¿ç•™
            fixed_messages.append(msg)

    return fixed_messages


async def langgraph_multi_agent(
    messages: List[Dict[str, Any]],
    canvas_id: str,
    session_id: str,
    text_model: ModelInfo,
    tool_list: List[ToolInfoJson],
    system_prompt: Optional[str] = None
) -> None:
    """å¤šæ™ºèƒ½ä½“å¤„ç†å‡½æ•°

    Args:
        messages: æ¶ˆæ¯å†å²
        canvas_id: ç”»å¸ƒID
        session_id: ä¼šè¯ID
        text_model: æ–‡æœ¬æ¨¡å‹é…ç½®
        tool_list: å·¥å…·æ¨¡å‹é…ç½®åˆ—è¡¨ï¼ˆå›¾åƒæˆ–è§†é¢‘æ¨¡å‹ï¼‰
        system_prompt: ç³»ç»Ÿæç¤ºè¯
    """
    print(f"\nğŸš€ å¼€å§‹å¤„ç†ä¼šè¯: {session_id}")
    print(f"ğŸ“‹ ä½¿ç”¨æ¨¡å‹: {text_model.get('provider')} - {text_model.get('model')}")
    print(f"ğŸ’¬ æ¶ˆæ¯æ•°é‡: {len(messages)}")
    print(f"ğŸ¨ å·¥å…·æ•°é‡: {len(tool_list)}")
    
    try:
        # 0. ä¿®å¤æ¶ˆæ¯å†å²
        print("ğŸ”§ ä¿®å¤æ¶ˆæ¯å†å²...")
        fixed_messages = _fix_chat_history(messages)
        print(f"âœ… æ¶ˆæ¯å†å²ä¿®å¤å®Œæˆï¼Œä¿®å¤åæ¶ˆæ¯æ•°: {len(fixed_messages)}")

        # 1. å‘é€å¼€å§‹å¤„ç†é€šçŸ¥
        print("ğŸ“¤ å‘é€å¼€å§‹å¤„ç†é€šçŸ¥åˆ°å‰ç«¯...")
        await send_to_websocket(session_id, cast(Dict[str, Any], {
            'type': 'info',
            'info': f'å¼€å§‹å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œä½¿ç”¨æ¨¡å‹: {text_model.get("model")}'
        }))

        # 2. åˆ›å»ºæ–‡æœ¬æ¨¡å‹
        print("ğŸ§  åˆ›å»ºæ–‡æœ¬æ¨¡å‹å®ä¾‹...")
        # æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯æ–°ç”»å¸ƒæˆ–ç©ºæ¶ˆæ¯åœºæ™¯
        is_new_session = len(fixed_messages) == 0
        text_model_instance = _create_text_model(text_model, is_new_session)
        print(f"âœ… æ–‡æœ¬æ¨¡å‹åˆ›å»ºæˆåŠŸ: {text_model.get('provider')} - {text_model.get('model')}")
        
        # æ–°å¢ï¼šåœ¨æ–°ä¼šè¯åœºæ™¯ä¸‹ï¼Œç›´æ¥è¿”å›æ¬¢è¿æ¶ˆæ¯ï¼Œé¿å…å®Œæ•´çš„æµå¤„ç†
        if is_new_session:
            print("ğŸš€ åœ¨æ–°ä¼šè¯åœºæ™¯ä¸‹ï¼Œç›´æ¥å‘é€æ¬¢è¿æ¶ˆæ¯å¹¶ç»“æŸå¤„ç†")
            await send_to_websocket(session_id, cast(Dict[str, Any], {
                'session_id': session_id,
                'type': 'stream',
                'content': "æ¬¢è¿ä½¿ç”¨Jaazï¼\n\nçœ‹èµ·æ¥æ‚¨è¿˜æ²¡æœ‰é…ç½®APIå¯†é’¥ã€‚è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½®æ‚¨çš„APIå¯†é’¥ï¼Œç„¶åæ‚¨å°±å¯ä»¥å¼€å§‹ä½¿ç”¨å®Œæ•´åŠŸèƒ½äº†ã€‚",
                'end': True
            }))
            print("âœ… æ¬¢è¿æ¶ˆæ¯å‘é€å®Œæˆï¼Œè·³è¿‡å®Œæ•´æµå¤„ç†æµç¨‹")
            return

        # 3. åˆ›å»ºæ™ºèƒ½ä½“
        print("ğŸ¤– åˆ›å»ºæ™ºèƒ½ä½“...")
        agents = AgentManager.create_agents(
            text_model_instance,
            tool_list,  # ä¼ å…¥æ‰€æœ‰æ³¨å†Œçš„å·¥å…·
            system_prompt or ""
        )
        agent_names = [agent.name for agent in agents]
        print(f'âœ… åˆ›å»ºçš„æ™ºèƒ½ä½“åˆ—è¡¨: {agent_names}')
        
        # 4. ç¡®å®šä¸Šä¸€ä¸ªæ´»è·ƒçš„æ™ºèƒ½ä½“
        print("ğŸ” æŸ¥æ‰¾ä¸Šä¸€ä¸ªæ´»è·ƒçš„æ™ºèƒ½ä½“...")
        last_agent = AgentManager.get_last_active_agent(
            fixed_messages, agent_names)
        print(f'âœ… ä¸Šä¸€ä¸ªæ´»è·ƒçš„æ™ºèƒ½ä½“: {last_agent if last_agent else "é»˜è®¤æ™ºèƒ½ä½“"}')

        # 5. åˆ›å»ºæ™ºèƒ½ä½“ç¾¤ç»„
        print("ğŸ‘¥ åˆ›å»ºæ™ºèƒ½ä½“ç¾¤ç»„...")
        swarm = create_swarm(
            agents=agents,  # type: ignore
            default_active_agent=last_agent if last_agent else agent_names[0]
        )
        print("âœ… æ™ºèƒ½ä½“ç¾¤ç»„åˆ›å»ºæˆåŠŸ")

        # 6. åˆ›å»ºä¸Šä¸‹æ–‡
        print("ğŸ“ åˆ›å»ºä¸Šä¸‹æ–‡...")
        context = {
            'canvas_id': canvas_id,
            'session_id': session_id,
            'tool_list': tool_list,
            'model': text_model.get('model'),  # æ·»åŠ æ¨¡å‹åç§°ï¼Œä¾¿äºStreamProcessorè¯†åˆ«Googleæ¨¡å‹
            'provider': text_model.get('provider'),  # æ·»åŠ æä¾›å•†ä¿¡æ¯
        }
        print(f"âœ… ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ: {context}")

        # 7. æµå¤„ç†
        print("ğŸ’¨ å¼€å§‹æµå¤„ç†...")
        processor = StreamProcessor(
            session_id, db_service, send_to_websocket, canvas_id)  # type: ignore
        print(f"âœ… æµå¤„ç†å™¨åˆ›å»ºæˆåŠŸï¼Œå‡†å¤‡å¤„ç†æµå¼å“åº”")
        await processor.process_stream(swarm, fixed_messages, context)
        print("âœ… æµå¤„ç†å®Œæˆ")

    except Exception as e:
        print(f"âŒ å¤„ç†ä¼šè¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        await _handle_error(e, session_id)


def _create_text_model(text_model: ModelInfo, is_new_session: bool = False) -> Any:
    """åˆ›å»ºè¯­è¨€æ¨¡å‹å®ä¾‹"""
    model = text_model.get('model')
    provider = text_model.get('provider')
    url = text_model.get('url')
    api_key = config_service.app_config.get(  # type: ignore
        provider, {}).get("api_key", "")

    # TODO: Verify if max token is working
    # max_tokens = text_model.get('max_tokens', 8148)

    if provider == 'ollama':
        return ChatOllama(
            model=model,
            base_url=url,
        )
    else:
        # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦ä¸ºç©º
        if not api_key:
            # æ–°å¢ï¼šåœ¨æ–°ä¼šè¯åœºæ™¯ä¸‹ä¸æŠ›å‡ºé”™è¯¯ï¼Œå…è®¸ç”¨æˆ·å…ˆä½“éªŒç•Œé¢
            if is_new_session:
                print("âš ï¸ åœ¨æ–°ä¼šè¯åœºæ™¯ä¸‹ç»•è¿‡APIå¯†é’¥æ£€æµ‹")
                # è¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ¨¡å‹å®ä¾‹ï¼Œé¿å…åœ¨ç©ºä¼šè¯æ—¶æŠ›å‡ºAPIå¯†é’¥é”™è¯¯
                class MockModel:
                    async def invoke(self, messages, **kwargs):
                        from langchain_core.messages import AIMessage
                        return AIMessage(content="æ¬¢è¿ä½¿ç”¨Jaazï¼\n\nçœ‹èµ·æ¥æ‚¨è¿˜æ²¡æœ‰é…ç½®APIå¯†é’¥ã€‚è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½®æ‚¨çš„APIå¯†é’¥ï¼Œç„¶åæ‚¨å°±å¯ä»¥å¼€å§‹ä½¿ç”¨å®Œæ•´åŠŸèƒ½äº†ã€‚")
                    
                    def bind_tools(self, tools, **kwargs):
                        # æ¨¡æ‹Ÿbind_toolsæ–¹æ³•ï¼Œè¿”å›selfä»¥æ”¯æŒé“¾å¼è°ƒç”¨
                        return self
                return MockModel()
            
            # å¦‚æœä¸æ˜¯æ–°ä¼šè¯ä¸”APIå¯†é’¥ä¸ºç©ºï¼ŒæŠ›å‡ºæ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
            raise ValueError(f"APIå¯†é’¥æœªè®¾ç½®: è¯·åœ¨è®¾ç½®ä¸­é…ç½®{provider}çš„APIå¯†é’¥")
            
        # Create httpx client with SSL configuration for ChatOpenAI
        http_client = get_http_client().create_httpx_client(provider_key=provider)
        http_async_client = get_http_client().create_async_httpx_client(provider_key=provider)
        return ChatOpenAI(
            model=model,
            api_key=api_key,  # type: ignore
            timeout=300,
            base_url=url,
            temperature=0,
            # max_tokens=max_tokens, # TODO: æš‚æ—¶æ³¨é‡Šæ‰æœ‰é—®é¢˜çš„å‚æ•°
            http_client=http_client,
            http_async_client=http_async_client
        )


async def _handle_error(error: Exception, session_id: str) -> None:
    """å¤„ç†é”™è¯¯"""
    print('Error in langgraph_agent', error)
    tb_str = traceback.format_exc()
    print(f"Full traceback:\n{tb_str}")
    traceback.print_exc()

    # ä½¿ç”¨é”™è¯¯å¤„ç†å·¥å…·æ¸…ç†é”™è¯¯æ¶ˆæ¯
    clean_error = clean_error_message(error)
    
    # è®°å½•æ¸…ç†åçš„é”™è¯¯æ¶ˆæ¯
    print(f"æ¸…ç†åçš„é”™è¯¯æ¶ˆæ¯: {clean_error}")

    # ç¡®ä¿é”™è¯¯æ¶ˆæ¯åŒ…å«session_idå­—æ®µï¼Œä»¥ä¾¿å‰ç«¯æ­£ç¡®å¤„ç†
    await send_to_websocket(session_id, cast(Dict[str, Any], {
        'session_id': session_id,
        'type': 'error',
        'error': clean_error
    }))
